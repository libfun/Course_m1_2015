% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
% % % % % % % % % % % % % % % % % % % %
% For Russian uncomment the following lines
% \usepackage[utf8x]{inputenc}
% \usepackage[russian]{babel}
% \def\keywordname{{\bf Ключевые слова:}}%
% % % % % % % % % % % % % % % % % % % % 

\begin{document}
%
\title{Supervised Learning for Link Prediction Using Similarity Indices}
%
\titlerunning{Supervised learning for link prediction}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Sergey Korolev\inst{1} \and Leonid Zhukov}
%
\authorrunning{Sergey Korolev et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Sergey Korolev, and Leonid Zhukov}
%
\institute{Higher School of Economics, Moscow, Russia,\\
\email{sokorolev@edu.hse.ru}\\ %WWW home page:
%\texttt{http://users/\homedir iekeland/web/welcome.html}
%\and
%Universit\'{e} de Paris-Sud,
%Laboratoire d'Analyse Num\'{e}rique, B\^{a}timent 425,\\
%F-91405 Orsay Cedex, France
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The abstract should summarize the contents of the paper
using at least 70 and at most 150 words. It will be set in 9-point
font size and be inset 1.0 cm from the right and left margins.
There will be two blank lines before and after the Abstract. \dots
\keywords{network analysis, graph theory, link prediction}
\end{abstract}
%
\section{Introduction}
%
Over the last few years the topic of link prediction has become very popular due to the rise of recommendation systems and social networks. The 

The aim of this paper is to try to devise an approach to predict the missing links in the network using only structural information of said network.
%
\section{Similarity Indices}
%
In this section, we will describe most popular local and global similarity indices and measure their accuracy in ranking links according to the probability that said links were removed from the network.

We'll start with describing the method for measuring said accuracy. We start with the set of edges $E$ of existing graph $G$. Then we shuffle it and split it into $n$ parts $E'_i, \ i \in \{1, \ldots, n\}$. Then for each step we subtract $E'_i$ from $E$ and look at the complement of the remaining set of edges $\bar{E_i}$ to the set of edges of complete graph on these nodes, so that $E^* = E \cup \bar{E} = E_i \cup E'_i \cup \bar{E} = E_i \cup \bar{E_i}$, where $E^*$ is the set of edges of complete graph and $\bar{E}$ is complement of graph $G$.

The main tool for measuring the accuracy of ranking is AUC score, which can be described as the probability that for pair of edges $(e_a, e_b) \ e_a \in E'_i, \ e_b \in \bar{E}$, the ranking is such that $score(e_a) > score(e_b)$ plus $0.5$ times the probability that $score(e_a) = score(e_b)$. Or as formula -- $AUC = \frac{n' + 0.5 n''}{n}$, where $n$ is number of pairs $(e_a, e_b) \ e_a \in E'_i, \ e_b \in \bar{E}$, $n'$ is number of pairs such that $score(e_a) > score(e_b)$ and $n''$ is number of pairs such that $score(e_a) = score(e_b)$.

We'll first describe all indices used for the prediction of links and then compare their AUC scores on different networks.
%
\subsection{Local Similarity Indices}
%
\subsubsection{Common neighbours (CN)}
%
This is the most obvious approach to the idea of capturing the similarity of two nodes in the network. As such, number of indices described below will use this measure with different weights. So if we denote $\Gamma(x)$ the number of neighbours of node $x$ in the graph, the formula looks like
\begin{equation}
s_{xy}^{CN} = |\Gamma(x) \cap \Gamma(y)|,
\end{equation}
where $s_{xy}$ is the index.
%
\subsubsection{Salton Index (SaI)}
%
This index is also called cosine similarity and is defined as
\begin{equation}
s_{xy}^{Salton} = \frac{|\Gamma(x) \cap \Gamma(y)|}{\sqrt{k_x \times k_x}},
\end{equation}
where $k_x$ is the degree of node $x$.
%
\subsubsection{Jaccard Index (JI)}
%
\begin{equation}
s_{xy}^{Jaccard} = \frac{|\Gamma(x) \cap \Gamma(y)|}{|\Gamma(x) \cup \Gamma(y)|}.
\end{equation}
%
\subsubsection{S{\o}rensen Index (SoI)}
%
\begin{equation}
s_{xy}^{S{\o}rensen} = \frac{2|\Gamma(x) \cap \Gamma(y)|}{k_x + k_y}.
\end{equation}
%
\subsubsection{Hub Promoted Index (HPI)}
%
\begin{equation}
s_{xy}^{HPI} = \frac{|\Gamma(x) \cap \Gamma(y)|}{min\{k_x, k_y\}}.
\end{equation}
%
\subsubsection{Hub Depressed Index (HDI)}
%
\begin{equation}
s_{xy}^{HDI} = \frac{|\Gamma(x) \cap \Gamma(y)|}{max\{k_x, k_y\}}.
\end{equation}
%
\subsubsection{Leicht–Holme–Newman Index (LHN1)}
%
\begin{equation}
s_{xy}^{LHN1} = \frac{|\Gamma(x) \cap \Gamma(y)|}{k_x \times k_y}.
\end{equation}
%
\subsubsection{Preferential Attachment Index (PAI)}
%
\begin{equation}
s_{xy}^{PA} = k_x \times k_y.
\end{equation}
%
\subsubsection{Adamic–Adar Index (AAI)}
%
\begin{equation}
s_{xy}^{AA} = \sum_{z \in \Gamma(x) \cap \Gamma(y)} \frac{1}{log k_z}.
\end{equation}
%
\subsubsection{Resource Allocation Index (RAI)}
%
\begin{equation}
s_{xy}^{RA} = \sum_{z \in \Gamma(x) \cap \Gamma(y)} \frac{1}{k_z}.
\end{equation}
%
\subsection{Global Similarity Indices}
%
\subsubsection{Katz Index (KI)}
%
\begin{equation}
S^{Katz} = (I - \beta A)^{-1} - I.
\end{equation}
%
\subsubsection{Leicht–Holme–Newman Index (LHN2)}
%
\begin{equation}
S^{LHN2} = 2 m \lambda_1 D^{-1} (I - \frac{\phi A}{\lambda_1})^{-1} D^{-1}.
\end{equation}
%
\subsubsection{Average Commute Time (ACT)}
%
\begin{equation}
S_{xy}^{ACT} = \frac{1}{l_{xx}^{+} + l_{yy}^{+} - 2l_{xy}^{+}}.
\end{equation}
%
\subsubsection{Cosine based on $L^+$ (CBL)}
%
\begin{equation}
s_{xy}^{cos^{+}} = cos(x, y)^{+} = \frac{v_x^T v_y}{|v_x| \cdot |v_y|} = \frac{l_{xy}^{+}}{\sqrt{l_{xx}^{+}\cdot l_{yy}^{+}}}.
\end{equation}
%
\subsubsection{Random Walk with Restart (RWR)}
%
\begin{equation}
s_{xy}^{RWR} = q_{xy} + q_{yx}.
\end{equation}
%
\subsubsection{Matrix Forest Index (MFI)}
%
\begin{equation}
S = (I + L)^{-1}.
\end{equation}
%
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.5}
\begin{table}
\begin{center}
\caption{AUC scores of local indices. AN - Word adjacencies network\cite{adjnoun-net}, CN - Neural network\cite{celneur-net}, Dp - Dolphin social network network\cite{dolph-net}, FB - American College football network\cite{foot-net}, LM - Les Miserables network\cite{lesmis-net}, PB - Books about US politics network\cite{polbook-net}, KC - Zachary's karate club network\cite{karate-net}, UA - US air transportation system network\cite{usair-net}. Best results for given network in bold.}
\begin{tabular}{| l | l | l | l | l | l | l | l | l | l | l |}
\hline
Net & CN & SaI & JI & SoI & HPI & HDI & LHN1 & PAI & AAI & RAI \\ \hline
AN & 0.662 & 0.606 & 0.603 & 0.603 & 0.618 & 0.603 & 0.566 & \textbf{0.744} & 0.662 & 0.659 \\ \hline
CN & 0.844 & 0.797 & 0.790 & 0.790 & 0.805 & 0.779 & 0.725 & 0.750 & 0.861 & \textbf{0.866} \\ \hline
Dp & 0.779 & 0.774 & 0.779 & 0.779 & 0.763 & 0.780 & 0.762 & 0.619 & \textbf{0.781} & \textbf{0.781} \\ \hline
FB & 0.846 & 0.856 & 0.858 & 0.858 & 0.856 & 0.857 & \textbf{0.859} & 0.270 & 0.846 & 0.846 \\ \hline
LM & 0.910 & 0.882 & 0.880 & 0.880 & 0.847 & 0.878 & 0.820 & 0.776 & 0.918 & \textbf{0.919} \\ \hline
PB & 0.887 & 0.884 & 0.875 & 0.875 & 0.894 & 0.863 & 0.848 & 0.653 & \textbf{0.897} & 0.890 \\ \hline
KC & 0.700 & 0.636 & 0.607 & 0.607 & 0.712 & 0.593 & 0.600 & 0.712 & 0.726 & \textbf{0.733} \\ \hline
UA & 0.934 & 0.908 & 0.897 & 0.897 & 0.869 & 0.891 & 0.767 & 0.885 & 0.945 & \textbf{0.951} \\ 
\hline
\end{tabular}
\end{center}
\end{table}
%
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.5}
\begin{table}
\begin{center}
\caption{AUC scores of global indices.}
\begin{tabular}{| l | l | l | l | l | l | l |}
\hline
Net & KI & LHN2 & ACT & CBL & RWR & MFI \\ \hline
AN & 0.714 & 0.549 & \textbf{0.742} & 0.586 & 0.738 & 0.667 \\ \hline
CN & 0.852 & 0.717 & 0.738 & 0.848 & 0.725 & \textbf{0.865} \\ \hline
Dp & 0.799 & \textbf{0.825} & 0.760 & 0.791 & 0.649 & 0.804 \\ \hline
FB & 0.857 & 0.879 & 0.588 & \textbf{0.885} & 0.273 & 0.878 \\ \hline
LM & \textbf{0.884} & 0.813 & 0.863 & 0.825 & 0.794 & 0.867 \\ \hline
PB & 0.891 & 0.858 & 0.729 & 0.891 & 0.618 & \textbf{0.899} \\ \hline
KC & \textbf{0.755} & 0.610 & 0.666 & 0.739 & 0.618 & 0.749 \\ \hline
UA & \textbf{0.920} & NaN & 0.892 & 0.913 & 0.862 & 0.913 \\ 
\hline
\end{tabular}
\end{center}
\end{table}
%
\section{Method}
%
We begin by defining the test dataset to check the performance of the algorithm. We shuffle the set of existing edges and split it into $n$ parts $E'_i, \ i \in \{1, \ldots, n\}$. It is said that $n = 10$ achieves the best complexity-precision tradeoff. Now we can use the set $\bar{E_i} = E'_i \cup \bar{E}$ as test dataset, where if $e \in E'_i$ then $class_e = 1$ and if $e \in \bar{E}$ then $class_e = 0$.

Now we need to construct training dataset to train our classifier on it and then check its performance on the test dataset. We take the remaining set of edges $E_i$, shuffle it and split it into $n$ parts $(E'_i)'_j$. To achieve balanced training dataset with the same amount of edges with class 0 and 1 we split the complement $\bar{E_i}$ into subsets of the same size as $(E'_i)'_j$. Now for each subset $(E'_i)'_j$ we take this subset out of $E_i$, then calculate indices on the complement of the remaining set and mark classes as $class_e$, where if $e \in (E'_i)'_j$ then $class_e = 1$ and if $e \in \bar{E}_i$ then $class_e = 0$. And to get balanced dataset, on each step we take edges from only one subset of $\bar{E_i}$ as examples of class 0.

After $n$ steps of the above process we get the trainig dataset of size $2 |E_i|$. Now we can train our classifier on this dataset and then test its performance on the test dataset. After testing SVM with linear kernel, decision trees, k nearest neighbours and random forests it was observed that the best performance is achieved with classification using random forests (as suggested in \cite{features-for-sl} and also tested in \cite{ipynb-sl}).
%
\section{Results}
%
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.5}
\begin{table}
\begin{center}
\caption{Scores of the algorithm performance on the class 1 of marked links.}
\begin{tabular}{| l | l | l | l | l | l |}
\hline
Net & F1 & Precision & Recall & ROCAUC & Accuracy  \\ \hline
AN & 0.027 $\pm$ 0.003 & 0.014 $\pm$ 0.002 & 0.579 $\pm$ 0.072 & 0.638 $\pm$ 0.032 & 0.696 $\pm$ 0.023  \\ \hline
CN & 0.041 $\pm$ 0.001 & 0.021 $\pm$ 0.000 & 0.811 $\pm$ 0.018 & 0.809 $\pm$ 0.008 & 0.808 $\pm$ 0.004  \\ \hline
Dp & 0.049 $\pm$ 0.008 & 0.025 $\pm$ 0.005 & 0.635 $\pm$ 0.082 & 0.704 $\pm$ 0.040 & 0.771 $\pm$ 0.035  \\ \hline
FB & 0.199 $\pm$ 0.025 & 0.116 $\pm$ 0.017 & 0.721 $\pm$ 0.044 & 0.831 $\pm$ 0.019 & 0.939 $\pm$ 0.011  \\ \hline
LM & 0.163 $\pm$ 0.021 & 0.090 $\pm$ 0.013 & 0.831 $\pm$ 0.068 & 0.875 $\pm$ 0.032 & 0.918 $\pm$ 0.013 \\ \hline
PB & 0.080 $\pm$ 0.009 & 0.042 $\pm$ 0.005 & 0.780 $\pm$ 0.080 & 0.812 $\pm$ 0.040 & 0.842 $\pm$ 0.012  \\ \hline
KC & 0.092 $\pm$ 0.026 & 0.049 $\pm$ 0.014 & 0.641 $\pm$ 0.145 & 0.717 $\pm$ 0.082 & 0.790 $\pm$  0.043 \\ \hline
UA & 0.083 $\pm$ 0.008 & 0.044 $\pm$ 0.004 & 0.901 $\pm$ 0.024 & 0.910 $\pm$ 0.011 & 0.919 $\pm$ 0.009  \\ 
\hline
\end{tabular}
\end{center}
\end{table}
%
\section{Conclusion}
%
Proposed approach of supervised learning for link prediction showed interesting results on the selected networks. It appears that the algoritm classifies most of the removed links correctly, but also classifies a lot of links that were not present in the network in the first place as the ones that were removed.
%
\\
\\
\\
%We assume that $H$ is
%$\left(A_{\infty},B_{\infty}\right)$-sub\-qua\-dra\-tic at infinity,
%for some constant symmetric matrices $A_{\infty}$ and $B_{\infty}$,
%with $B_{\infty}-A_{\infty}$ positive definite. Set:
%\begin{eqnarray}
%\gamma :&=&{\rm smallest\ eigenvalue\ of}\ \ B_{\infty} - A_{\infty} \\
%  \lambda : &=& {\rm largest\ negative\ eigenvalue\ of}\ \
%  J \frac{d}{dt} +A_{\infty}\ .
%\end{eqnarray}
%
%Theorem~\ref{ghou:pre} tells us that if $\lambda +\gamma < 0$, the
%boundary-value problem:
%\begin{equation}
%\begin{array}{rcl}
%  \dot{x}&=&JH' (x)\\
%  x(0)&=&x (T)
%\end{array}
%\end{equation}
%has at least one solution
%$\overline{x}$, which is found by minimizing the dual
%action functional:
%\begin{equation}
%  \psi (u) = \int_{o}^{T} \left[\frac{1}{2}
%  \left(\Lambda_{o}^{-1} u,u\right) + N^{\ast} (-u)\right] dt
%\end{equation}
%on the range of $\Lambda$, which is a subspace $R (\Lambda)_{L}^{2}$
%with finite codimension. Here
%\begin{equation}
%  N(x) := H(x) - \frac{1}{2} \left(A_{\infty} x,x\right)
%\end{equation}
%is a convex function, and
%\begin{equation}
%  N(x) \le \frac{1}{2}
%  \left(\left(B_{\infty} - A_{\infty}\right) x,x\right)
%  + c\ \ \ \forall x\ .
%\end{equation}
%
%%
%\begin{proposition}
%Assume $H'(0)=0$ and $ H(0)=0$. Set:
%\begin{equation}
%  \delta := \liminf_{x\to 0} 2 N (x) \left\|x\right\|^{-2}\ .
%  \label{eq:one}
%\end{equation}
%
%If $\gamma < - \lambda < \delta$,
%the solution $\overline{u}$ is non-zero:
%\begin{equation}
%  \overline{x} (t) \ne 0\ \ \ \forall t\ .
%\end{equation}
%\end{proposition}
%%
%\begin{proof}
%Condition (\ref{eq:one}) means that, for every
%$\delta ' > \delta$, there is some $\varepsilon > 0$ such that
%\begin{equation}
%  \left\|x\right\| \le \varepsilon \Rightarrow N (x) \le
%  \frac{\delta '}{2} \left\|x\right\|^{2}\ .
%\end{equation}
%
%It is an exercise in convex analysis, into which we shall not go, to
%show that this implies that there is an $\eta > 0$ such that
%\begin{equation}
%  f\left\|x\right\| \le \eta
%  \Rightarrow N^{\ast} (y) \le \frac{1}{2\delta '}
%  \left\|y\right\|^{2}\ .
%  \label{eq:two}
%\end{equation}
%
%\begin{figure}
%\vspace{2.5cm}
%\caption{This is the caption of the figure displaying a white eagle and
%a white horse on a snow field}
%\end{figure}
%
%Since $u_{1}$ is a smooth function, we will have
%$\left\|hu_{1}\right\|_\infty \le \eta$
%for $h$ small enough, and inequality (\ref{eq:two}) will hold,
%yielding thereby:
%\begin{equation}
%  \psi (hu_{1}) \le \frac{h^{2}}{2}
%  \frac{1}{\lambda} \left\|u_{1} \right\|_{2}^{2} + \frac{h^{2}}{2}
%  \frac{1}{\delta '} \left\|u_{1}\right\|^{2}\ .
%\end{equation}
%
%If we choose $\delta '$ close enough to $\delta$, the quantity
%$\left(\frac{1}{\lambda} + \frac{1}{\delta '}\right)$
%will be negative, and we end up with
%\begin{equation}
%  \psi (hu_{1}) < 0\ \ \ \ \ {\rm for}\ \ h\ne 0\ \ {\rm small}\ .
%\end{equation}
%
%On the other hand, we check directly that $\psi (0) = 0$. This shows
%that 0 cannot be a minimizer of $\psi$, not even a local one.
%So $\overline{u} \ne 0$ and
%$\overline{u} \ne \Lambda_{o}^{-1} (0) = 0$. \qed
%\end{proof}
%%
%\begin{corollary}
%Assume $H$ is $C^{2}$ and
%$\left(a_{\infty},b_{\infty}\right)$-subquadratic at infinity. Let
%$\xi_{1},\allowbreak\dots,\allowbreak\xi_{N}$  be the
%equilibria, that is, the solutions of $H' (\xi ) = 0$.
%Denote by $\omega_{k}$
%the smallest eigenvalue of $H'' \left(\xi_{k}\right)$, and set:
%\begin{equation}
%  \omega : = {\rm Min\,} \left\{\omega_{1},\dots,\omega_{k}\right\}\ .
%\end{equation}
%If:
%\begin{equation}
%  \frac{T}{2\pi} b_{\infty} <
%  - E \left[- \frac{T}{2\pi}a_{\infty}\right] <
%  \frac{T}{2\pi}\omega
%  \label{eq:three}
%\end{equation}
%then minimization of $\psi$ yields a non-constant $T$-periodic solution
%$\overline{x}$.
%\end{corollary}
%%
%
%We recall once more that by the integer part $E [\alpha ]$ of
%$\alpha \in \bbbr$, we mean the $a\in \bbbz$
%such that $a< \alpha \le a+1$. For instance,
%if we take $a_{\infty} = 0$, Corollary 2 tells
%us that $\overline{x}$ exists and is
%non-constant provided that:
%
%\begin{equation}
%  \frac{T}{2\pi} b_{\infty} < 1 < \frac{T}{2\pi}
%\end{equation}
%or
%\begin{equation}
%  T\in \left(\frac{2\pi}{\omega},\frac{2\pi}{b_{\infty}}\right)\ .
%  \label{eq:four}
%\end{equation}
%
%%
%\begin{proof}
%The spectrum of $\Lambda$ is $\frac{2\pi}{T} \bbbz +a_{\infty}$. The
%largest negative eigenvalue $\lambda$ is given by
%$\frac{2\pi}{T}k_{o} +a_{\infty}$,
%where
%\begin{equation}
%  \frac{2\pi}{T}k_{o} + a_{\infty} < 0
%  \le \frac{2\pi}{T} (k_{o} +1) + a_{\infty}\ .
%\end{equation}
%Hence:
%\begin{equation}
%  k_{o} = E \left[- \frac{T}{2\pi} a_{\infty}\right] \ .
%\end{equation}
%
%The condition $\gamma < -\lambda < \delta$ now becomes:
%\begin{equation}
%  b_{\infty} - a_{\infty} <
%  - \frac{2\pi}{T} k_{o} -a_{\infty} < \omega -a_{\infty}
%\end{equation}
%which is precisely condition (\ref{eq:three}).\qed
%\end{proof}
%%
%
%\begin{lemma}
%Assume that $H$ is $C^{2}$ on $\bbbr^{2n} \setminus \{ 0\}$ and
%that $H'' (x)$ is non-de\-gen\-er\-ate for any $x\ne 0$. Then any local
%minimizer $\widetilde{x}$ of $\psi$ has minimal period $T$.
%\end{lemma}
%%
%\begin{proof}
%We know that $\widetilde{x}$, or
%$\widetilde{x} + \xi$ for some constant $\xi
%\in \bbbr^{2n}$, is a $T$-periodic solution of the Hamiltonian system:
%\begin{equation}
%  \dot{x} = JH' (x)\ .
%\end{equation}
%
%There is no loss of generality in taking $\xi = 0$. So
%$\psi (x) \ge \psi (\widetilde{x} )$
%for all $\widetilde{x}$ in some neighbourhood of $x$ in
%$W^{1,2} \left(\bbbr / T\bbbz ; \bbbr^{2n}\right)$.
%
%But this index is precisely the index
%$i_{T} (\widetilde{x} )$ of the $T$-periodic
%solution $\widetilde{x}$ over the interval
%$(0,T)$, as defined in Sect.~2.6. So
%\begin{equation}
%  i_{T} (\widetilde{x} ) = 0\ .
%  \label{eq:five}
%\end{equation}
%
%Now if $\widetilde{x}$ has a lower period, $T/k$ say,
%we would have, by Corollary 31:
%\begin{equation}
%  i_{T} (\widetilde{x} ) =
%  i_{kT/k}(\widetilde{x} ) \ge
%  ki_{T/k} (\widetilde{x} ) + k-1 \ge k-1 \ge 1\ .
%\end{equation}
%
%This would contradict (\ref{eq:five}), and thus cannot happen.\qed
%\end{proof}
%%
%\paragraph{Notes and Comments.}
%The results in this section are a
%refined version of \cite{clar:eke};
%the minimality result of Proposition
%14 was the first of its kind.
%
%To understand the nontriviality conditions, such as the one in formula
%(\ref{eq:four}), one may think of a one-parameter family
%$x_{T}$, $T\in \left(2\pi\omega^{-1}, 2\pi b_{\infty}^{-1}\right)$
%of periodic solutions, $x_{T} (0) = x_{T} (T)$,
%with $x_{T}$ going away to infinity when $T\to 2\pi \omega^{-1}$,
%which is the period of the linearized system at 0.
%
%\begin{table}
%\caption{This is the example table taken out of {\it The
%\TeX{}book,} p.\,246}
%\begin{center}
%\begin{tabular}{r@{\quad}rl}
%\hline
%\multicolumn{1}{l}{\rule{0pt}{12pt}
%                   Year}&\multicolumn{2}{l}{World population}\\[2pt]
%\hline\rule{0pt}{12pt}
%8000 B.C.  &     5,000,000& \\
%  50 A.D.  &   200,000,000& \\
%1650 A.D.  &   500,000,000& \\
%1945 A.D.  & 2,300,000,000& \\
%1980 A.D.  & 4,400,000,000& \\[2pt]
%\hline
%\end{tabular}
%\end{center}
%\end{table}
%%
%\begin{theorem} [Ghoussoub-Preiss]\label{ghou:pre}
%Assume $H(t,x)$ is
%$(0,\varepsilon )$-subquadratic at
%infinity for all $\varepsilon > 0$, and $T$-periodic in $t$
%\begin{equation}
%  H (t,\cdot )\ \ \ \ \ {\rm is\ convex}\ \ \forall t
%\end{equation}
%\begin{equation}
%  H (\cdot ,x)\ \ \ \ \ {\rm is}\ \ T{\rm -periodic}\ \ \forall x
%\end{equation}
%\begin{equation}
%  H (t,x)\ge n\left(\left\|x\right\|\right)\ \ \ \ \
%  {\rm with}\ \ n (s)s^{-1}\to \infty\ \ {\rm as}\ \ s\to \infty
%\end{equation}
%\begin{equation}
%  \forall \varepsilon > 0\ ,\ \ \ \exists c\ :\
%  H(t,x) \le \frac{\varepsilon}{2}\left\|x\right\|^{2} + c\ .
%\end{equation}
%
%Assume also that $H$ is $C^{2}$, and $H'' (t,x)$ is positive definite
%everywhere. Then there is a sequence $x_{k}$, $k\in \bbbn$, of
%$kT$-periodic solutions of the system
%\begin{equation}
%  \dot{x} = JH' (t,x)
%\end{equation}
%such that, for every $k\in \bbbn$, there is some $p_{o}\in\bbbn$ with:
%\begin{equation}
%  p\ge p_{o}\Rightarrow x_{pk} \ne x_{k}\ .
%\end{equation}
%\qed
%\end{theorem}
%%
%\begin{example} [{{\rm External forcing}}]
%Consider the system:
%\begin{equation}
%  \dot{x} = JH' (x) + f(t)
%\end{equation}
%where the Hamiltonian $H$ is
%$\left(0,b_{\infty}\right)$-subquadratic, and the
%forcing term is a distribution on the circle:
%\begin{equation}
%  f = \frac{d}{dt} F + f_{o}\ \ \ \ \
%  {\rm with}\ \ F\in L^{2} \left(\bbbr / T\bbbz; \bbbr^{2n}\right)\ ,
%\end{equation}
%where $f_{o} : = T^{-1}\int_{o}^{T} f (t) dt$. For instance,
%\begin{equation}
%  f (t) = \sum_{k\in \bbbn} \delta_{k} \xi\ ,
%\end{equation}
%where $\delta_{k}$ is the Dirac mass at $t= k$ and
%$\xi \in \bbbr^{2n}$ is a
%constant, fits the prescription. This means that the system
%$\dot{x} = JH' (x)$ is being excited by a
%series of identical shocks at interval $T$.
%\end{example}
%%
%\begin{definition}
%Let $A_{\infty} (t)$ and $B_{\infty} (t)$ be symmetric
%operators in $\bbbr^{2n}$, depending continuously on
%$t\in [0,T]$, such that
%$A_{\infty} (t) \le B_{\infty} (t)$ for all $t$.
%
%A Borelian function
%$H: [0,T]\times \bbbr^{2n} \to \bbbr$
%is called
%$\left(A_{\infty} ,B_{\infty}\right)$-{\it subquadratic at infinity}
%if there exists a function $N(t,x)$ such that:
%\begin{equation}
%  H (t,x) = \frac{1}{2} \left(A_{\infty} (t) x,x\right) + N(t,x)
%\end{equation}
%\begin{equation}
%  \forall t\ ,\ \ \ N(t,x)\ \ \ \ \
%  {\rm is\ convex\ with\  respect\  to}\ \ x
%\end{equation}
%\begin{equation}
%  N(t,x) \ge n\left(\left\|x\right\|\right)\ \ \ \ \
%  {\rm with}\ \ n(s)s^{-1}\to +\infty\ \ {\rm as}\ \ s\to +\infty
%\end{equation}
%\begin{equation}
%  \exists c\in \bbbr\ :\ \ \ H (t,x) \le
%  \frac{1}{2} \left(B_{\infty} (t) x,x\right) + c\ \ \ \forall x\ .
%\end{equation}
%
%If $A_{\infty} (t) = a_{\infty} I$ and
%$B_{\infty} (t) = b_{\infty} I$, with
%$a_{\infty} \le b_{\infty} \in \bbbr$,
%we shall say that $H$ is
%$\left(a_{\infty},b_{\infty}\right)$-subquadratic
%at infinity. As an example, the function
%$\left\|x\right\|^{\alpha}$, with
%$1\le \alpha < 2$, is $(0,\varepsilon )$-subquadratic at infinity
%for every $\varepsilon > 0$. Similarly, the Hamiltonian
%\begin{equation}
%H (t,x) = \frac{1}{2} k \left\|k\right\|^{2} +\left\|x\right\|^{\alpha}
%\end{equation}
%is $(k,k+\varepsilon )$-subquadratic for every $\varepsilon > 0$.
%Note that, if $k<0$, it is not convex.
%\end{definition}
%%
%
%\paragraph{Notes and Comments.}
%The first results on subharmonics were
%obtained by Rabinowitz in \cite{rab}, who showed the existence of
%infinitely many subharmonics both in the subquadratic and superquadratic
%case, with suitable growth conditions on $H'$. Again the duality
%approach enabled Clarke and Ekeland in \cite{clar:eke:2} to treat the
%same problem in the convex-subquadratic case, with growth conditions on
%$H$ only.
%
%Recently, Michalek and Tarantello (see \cite{mich:tar} and \cite{tar})
%have obtained lower bound on the number of subharmonics of period $kT$,
%based on symmetry considerations and on pinching estimates, as in
%Sect.~5.2 of this article.

%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
%\bibitem {clar:eke}
%Clarke, F., Ekeland, I.:
%Nonlinear oscillations and
%boundary-value problems for Hamiltonian systems.
%Arch. Rat. Mech. Anal. 78, 315--333 (1982)
%
%\bibitem {clar:eke:2}
%Clarke, F., Ekeland, I.:
%Solutions p\'{e}riodiques, du
%p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
%Note CRAS Paris 287, 1013--1015 (1978)
%
%\bibitem {mich:tar}
%Michalek, R., Tarantello, G.:
%Subharmonic solutions with prescribed minimal
%period for nonautonomous Hamiltonian systems.
%J. Diff. Eq. 72, 28--55 (1988)
%
%\bibitem {tar}
%Tarantello, G.:
%Subharmonic solutions for Hamiltonian
%systems via a $\bbbz_{p}$ pseudoindex theory.
%Annali di Matematica Pura (to appear)
%
%\bibitem {rab}
%Rabinowitz, P.:
%On subharmonic solutions of a Hamiltonian system.
%Comm. Pure Appl. Math. 33, 609--633 (1980) 

\bibitem {features-for-sl}
Cukierski, W., Hamner, B., Yang, B.:
Graph-based Features for Supervised Link Prediction.
Proceedings of International Joint Conference on Neural Networks, San Jose, California, USA (2011)

\bibitem {adjnoun-net}
Newman, M. E. J.
Phys. Rev. E 74, 036104 (2006). 
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {celneur-net}
Watts, D. J., Strogatz, S. H.
Nature 393, 440--442 (1998). 
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {dolph-net}
Lusseau, D., Schneider, K., Boisseau, O. J., Haase, P., Slooten, E., Dawson, S. M.
Behavioral Ecology and Sociobiology 54, 396--405 (2003). 
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {foot-net}
Girvan, M., Newman, M. E. J.
Proc. Natl. Acad. Sci. USA 99, 7821--7826 (2002). 
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {lesmis-net}
Knuth, D. E.
The Stanford GraphBase: A Platform for Combinatorial Computing, Addison-Wesley, Reading, MA (1993).
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {polbook-net}
Krebs, V.
Datasets. 
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {karate-net}
Zachary, W. W.:
An information flow model for conflict and fission in small groups.
Journal of Anthropological Research 33, 452--473 (1977). 
Available at: \texttt{http://www-personal.umich.edu/\~{}mejn/netdata/}.

\bibitem {usair-net}
Batageli, V., Mrvar, A.
Pajek datasets. 
Available at: \texttt{http://vlado.fmf.uni-lj.si/pub/networks/data/default.htm}.

\bibitem {ipynb-sl}
\texttt{http://github.com/libfun/...}

\end{thebibliography}

\end{document}
